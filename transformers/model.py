from torch import nn, Tensor


def attention():

    pass


class Transformer(nn.Module):
    
    def __init__(self):
        super().__init__()
        


    def forward(self):
        ...


class SelfAttention(nn.Module):

    def __init__(self) -> None:
        super().__init__()

    def forward(self, x):
        ...


class Head(nn.Module):

    def __init__(self) -> None:
        super().__init__(*args, **kwargs)

    def forward(self):
        ...


